{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5265c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17cc8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from email import message_from_file\n",
    "from email.utils import parseaddr,getaddresses\n",
    "from email.header import decode_header, make_header\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bf0c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls(text):\n",
    "    url_pattern = r'\\b(?:https?://|www\\.)\\S+\\b'\n",
    "    return re.findall(url_pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d63511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(path):\n",
    "    with open(path,\"r\",encoding=\"utf-8\",errors=\"ignore\") as f:\n",
    "        msg=message_from_file(f)\n",
    "    name,addr=parseaddr(msg.get(\"From\",\"\"))\n",
    "    subject=str(make_header(decode_header(msg.get(\"Subject\",\"\"))))\n",
    "\n",
    "    body=\"\"\n",
    "    if msg.is_multipart():\n",
    "        for part in msg.walk():\n",
    "            c_type=part.get_content_type()\n",
    "            disp=part.get_content_disposition()\n",
    "\n",
    "            if c_type==\"text/plain\" and disp is None:\n",
    "                body=part.get_payload(decode=True).decode(\n",
    "                msg.get_content_charset(\"utf-8\"),errors=\"ignore\"\n",
    "                )\n",
    "                break\n",
    "            \n",
    "            elif c_type==\"text/html\" and not body:\n",
    "                html=part.get_payload(decode=True).decode(\n",
    "                    part.get_content_charset(\"utf-8\"),errors=\"ignore\"\n",
    "                )\n",
    "                body=BeautifulSoup(html,\"html.parser\").get_text(\" \",strip=True)\n",
    "\n",
    "    else:\n",
    "        body=msg.get_payload(decode=True).decode(\n",
    "            msg.get_content_charset(\"utf-8\"),errors=\"ignore\"\n",
    "        )\n",
    "\n",
    "    urls=extract_urls(body)\n",
    "    has_url=1 if len(urls)>0 else 0\n",
    "    num_urls=len(urls)\n",
    "\n",
    "    cc_list=getaddresses(msg.get_all(\"Cc\",[]))\n",
    "    bcc_list=getaddresses(msg.get_all(\"Bcc\",[]))\n",
    "    num_cc=len(cc_list)\n",
    "    num_bcc=len(bcc_list)\n",
    "\n",
    "    sender_domain = addr.split(\"@\")[-1] if \"@\" in addr else \"\"\n",
    "\n",
    "    data_dict = {\n",
    "        \"From\": addr,\n",
    "        \"Subject\": subject,\n",
    "        \"Body\": body,\n",
    "        \"URLs\": urls,\n",
    "        \"SenderDomain\": sender_domain,\n",
    "        \"HasURL\": has_url,\n",
    "        \"NumURLs\": num_urls,\n",
    "        \"NumCC\": num_cc,\n",
    "        \"NumBCC\": num_bcc\n",
    "    }\n",
    "\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3fe622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tldextract\n",
    "\n",
    "def url_data(url_list):\n",
    "    url_dicts=[]\n",
    "\n",
    "    for url in url_list:\n",
    "        extracted=tldextract.extract(url)\n",
    "        domain=extracted.domain\n",
    "        sub_domain=extracted.subdomain\n",
    "        tld=extracted.suffix\n",
    "        path=url.split(f\"{domain}.{tld}\")[-1] if domain and tld else \"\"\n",
    "\n",
    "        url_info={\n",
    "            \"URL\": url,\n",
    "            \"Domain\": domain,\n",
    "            \"Subdomain\": sub_domain,\n",
    "            \"TLD\": tld,\n",
    "            \"Path\": path,\n",
    "            \"Length\": len(url),\n",
    "            \"NumSpecialChars\": sum(1 for c in url if not c.isalnum())\n",
    "        }\n",
    "\n",
    "        url_dicts.append(url_info)\n",
    "        \n",
    "    return url_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61901327",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"F:/college/bca_college/sixth sem/project/code/data_files/easy_ham/0010.4996141de3f21e858c22f88231a9f463\"\n",
    "\n",
    "email=extract_data(path)\n",
    "url=url_data(email[\"URLs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edfdeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing 00002.9438920e9a55591b18e60d1ed37d992b: unknown encoding: default\n",
      "Error processing 00003.590eff932f8704d8b0fcbe69d023b54d: unknown encoding: default\n",
      "Error processing 00004.bdcc075fa4beb5157b5dd6cd41d8887b: unknown encoding: default\n",
      "Error processing 00005.ed0aba4d386c5e62bc737cf3f0ed9589: unknown encoding: default\n",
      "Error processing 00006.3ca1f399ccda5d897fecb8c57669a283: unknown encoding: chinesebig5\n",
      "Error processing 00106.09988f439b8547dc90efb1530c02329b: unknown encoding: default_charset\n",
      "Error processing 00108.813fc6306b631b5c58ecfc26caf3a8dc: unknown encoding: default_charset\n",
      "Error processing 00293.2503973d5b437aa173b6dd97e6f14202: unknown encoding: default\n",
      "Error processing 00324.272b1fb3642d24e21dac949444b21e65: unknown encoding: default_charset\n",
      "Error processing 00352.206639789c6ba89977375c62856f20fc: unknown encoding: default\n",
      "Error processing 00380.717154ebf88ae594956736cc50bdeaf4: unknown encoding: default\n",
      "Error processing 00395.74aee42fac915ca758047506ec59a21f: unknown encoding: default_charset\n",
      "Error processing 00409.1faf0d6f87e8b70f0bb05b9040d56fca: unknown encoding: default\n",
      "Error processing 00478.a4d1f3bcbb571f1c3809bf47cb5ee57f: unknown encoding: default\n",
      "Error processing 00573.f33cc9f9253eda8eceaa7ace8f1a0f50: unknown encoding: default\n",
      "Error processing 00579.d94454f0e596c00bf22ce1f315427143: unknown encoding: default_charset\n",
      "Error processing 00580.c3b23134b4767f5e796d0df997fede33: unknown encoding: default_charset\n",
      "Error processing 00760.254b8986f3d7b6cbda1cc7ce16860e6c: unknown encoding: default\n",
      "Error processing 00824.eec96f74d95afedbe574498808d29395: unknown encoding: gb2312_charset\n",
      "Error processing 00941.3ad67a2e6c3bc19d2187dd5a98e05c9d: unknown encoding: default_charset\n",
      "Error processing 00983.753f8ed9cf897cce13c3d5358f2d77d4: unknown encoding: default\n",
      "Error processing 01007.255b826a1098e8b7d603c7dcf79f3fba: unknown encoding: default_charset\n",
      "Error processing 01220.e399b9acc45a23ec853a9bb1d2a1a859: unknown encoding: default\n",
      "Email DataFrame shape: (4166, 9)\n",
      "URL DataFrame shape: (30501, 8)\n"
     ]
    }
   ],
   "source": [
    "folders = [\n",
    "    (\"F:/college/bca_college/sixth sem/project/code/data_files/spam_2\", \"spam\"),\n",
    "    (\"F:/college/bca_college/sixth sem/project/code/data_files/easy_ham\", \"ham\"),\n",
    "    (\"F:/college/bca_college/sixth sem/project/code/data_files/hard_ham\", \"ham\")\n",
    "]\n",
    "\n",
    "email_data_list = []\n",
    "url_data_list = []\n",
    "\n",
    "for folder_path, label in folders:\n",
    "    for filename in os.listdir(folder_path):\n",
    "        filepath = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(filepath):\n",
    "            try:\n",
    "                # Extract email-level data\n",
    "                email_dict = extract_data(filepath)\n",
    "                email_dict_copy = {k:v for k,v in email_dict.items() if k != \"URLs\"}\n",
    "                email_dict_copy[\"Label\"] = label\n",
    "                email_data_list.append(email_dict_copy)\n",
    "                \n",
    "                # Extract URL-level data\n",
    "                urls = email_dict.get(\"URLs\", [])\n",
    "                url_dicts = url_data(urls)\n",
    "                for u in url_dicts:\n",
    "                    u[\"Label\"] = label\n",
    "                url_data_list.extend(url_dicts)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Create DataFrames\n",
    "df_emails = pd.DataFrame(email_data_list)\n",
    "df_urls = pd.DataFrame(url_data_list)\n",
    "\n",
    "# Save CSVs\n",
    "df_emails.to_csv(\"email_dataset.csv\", index=False)\n",
    "df_urls.to_csv(\"url_dataset.csv\", index=False)\n",
    "\n",
    "print(\"Email DataFrame shape:\", df_emails.shape)\n",
    "print(\"URL DataFrame shape:\", df_urls.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (email venv)",
   "language": "python",
   "name": "old_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
